{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-3a7b47819c10>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-3a7b47819c10>\"\u001b[1;36m, line \u001b[1;32m47\u001b[0m\n\u001b[1;33m    d_b4 = tf.get_variable('d_b4', [1], initializer = tf.constant_initializer(0))\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def discriminator(x_image, reuse=False):\n",
    "    if (reuse):\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    #first conv and pool layers\n",
    "    #search for 32 different 5x5 pixel features\n",
    "    #create weight and bias variables w/ tf.get_variable\n",
    "    #first weight matrix will be randomly initialized and will be 5x5\n",
    "    d_w1 = tf.get_variable('d_w1', [5,5,1,32], initializer =tf.truncated_normal_initializer(stdev=0.02))\n",
    "    \n",
    "    #tf.constant_init generates tensors with constant values\n",
    "    d_b1 = tf.get_variable('d_b1', [32], initializer= tf.constant_initializer(0))\n",
    "    #tf.nn.conv2d() is tf's function for a common convolution\n",
    "    #it takes 4 arguments: input volume(28x28x1), weight matrix, stride, and padding\n",
    "    #strides = [batch,height,width,channels]\n",
    "    \n",
    "    d1= tf.nn.conv2d(input=x_image, filter =d_w1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    d1 = d1 + d_b1\n",
    "    #squash with relu\n",
    "    d1= tf.nn.relu(d1)\n",
    "    #a pooling layer performs down-sampling by dividing the input into\n",
    "    #into rectangular pooling regions and computing the average of each region\n",
    "    d1=tf.nn.avg_pool(d1, ksize=[1,2,2,1], strides =[1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    #Second convolutional and pool layers\n",
    "    \n",
    "    #these search for 64 5 x5 features\n",
    "    \n",
    "    d_w2 = tf.get_variable('d_w2', [5,5,32,64], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    d_b2= tf.get_variable('d_b2', [64], initializer=tf.constant_initializer(0))\n",
    "    d2 = tf.nn.conv2d(input = d1, filter=d_w2, strides =[1,1,1,1], padding ='SAME')\n",
    "    d2 = d2 + d_b2\n",
    "    d2 = tf.nn.relu(d2)\n",
    "    d2 = tf.nn.avg_pool(d2, ksize =[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    #now fully connected layer one\n",
    "    \n",
    "    d_w3 = tf.get_variable('d_w3', [7*7*64,1024], initializer=tf.truncated_normal_initializer(stddev(0.02)))\n",
    "    d_b3= tf.get_variable('d_b3', [1024], initializer=tf.constant_initializer(0))\n",
    "    d3 = tf.reshape(d2, [-1,7*7*64])\n",
    "    d3 = tf.matmul(d3, d_w3)\n",
    "    d3= d3 + d_b3\n",
    "    d3 = tf.nn.relu(d3)\n",
    "    \n",
    "    #Last fully connected layer and has output\n",
    "    \n",
    "    d_w4 = tf.get_variable('d_w4', [1024,1],initializer=tf.truncated_normal_initializer(stddev(0.02))\n",
    "    d_b4 = tf.get_variable('d_b4', [1], initializer = tf.constant_initializer(0))\n",
    "    \n",
    "    #do a final matrix  multiplication and return the activation value\n",
    "    \n",
    "                           \n",
    "    d4 = tf.matmul(d3, d_w4) + d_b4\n",
    "    \n",
    "    return d4\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-0aabb4d2c974>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-0aabb4d2c974>\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    g_w4 = tf.get_variable('g_w4', [1,1,z_dim/4,1] dtype =tf.float32, initializer= tf.truncated_normal_initializer(stddev=0.02))\u001b[0m\n\u001b[1;37m                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#The generator is like a reverse CNN, with CNNs the goal is to\n",
    "#transform a 2 or 3 dimensional matrix of pixel values into a single probability\n",
    "#a generator takes an n-dimensional noise vector and upsample it\n",
    "#to become(in this case) a 28 x 28 image\n",
    "#Relus are used to stabilize the outputs of each layer\n",
    "\n",
    "def generator(batch_size, z_dim):\n",
    "    z=tf.truncated_normal([batch_size, z_dim], mean = 0, stddev=1, name='z')\n",
    "    #first deconv block\n",
    "    g_w1 = tf.get_variable('g_w1', [z_dim, 3136], dtype=tf.float32, initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b1 = tf.get_variable('g_b1',[3136], initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g1 = tf.reshape(g1, [-1,56,56,1])\n",
    "    g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='bn1')\n",
    "    g1 = tf.nn.relu(g1)\n",
    "    \n",
    "    #generate 50 features\n",
    "    \n",
    "    g_w2 = tf.get_variable('g_w2', [3,3,1,z_dim/2], dtype=tf.float32, initiializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b2 = tf.get_variable('g_b2',[z_dim/2], initializer =tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g2 = tf.nn.conv2d(g1,g_w2, strides=[1,2,2,1], padding = 'SAME')\n",
    "    g2 = g2 + g_b2\n",
    "    g2 = tf.contrib.layers.batch_norm(g2, epsilon=1e-5, scope ='bn2')\n",
    "    g2 = tf.nn.relu(g2)\n",
    "    g2 = tf.image.resize_image(g2,[56,56])\n",
    "    \n",
    "    #generate 25 features\n",
    "    \n",
    "    g_w3 = tf.get_variable('g_w3', [3,3,z_dim/2,z_dim/4], dtype=tf.float32, initializer = tf.truncated_normal_initializer(stddev(0.02)))\n",
    "    g_b3 = tf.get_variable('g_b4', [z_dim/4], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g3 = tf.nn.conv2d(g2, g_w3, strides=[1,2,2,1], padding = 'SAME')\n",
    "    g3 = g3 + g_b3\n",
    "    g3 = tf.contrib.layers.batch_norm(g3, epsilon=1e-5, scope = 'bn3')\n",
    "    g3 = tf.nn.relu(g3)\n",
    "    g3 = tf.image.resize_images(g3, [56,56])\n",
    "    \n",
    "    #final leyer with output\n",
    "    \n",
    "    g_w4 = tf.get_variable('g_w4', [1,1,z_dim/4,1] dtype =tf.float32, initializer= tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b4 = tf.get_variable('g_b4', [1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g4 = tf.nn.conv2d(g3, g_w4, strides=[1,2,2,1], padding = 'SAME')\n",
    "    g4 = g4 + g_b4\n",
    "    g4 = tf.sigmoid(g4)\n",
    "    \n",
    "    #no batch normalization and add a sigmoid function to make generated images crisper\n",
    "    #dimensions of g4 = batch_size x 28 x 28 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-7e890958d9ed>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-7e890958d9ed>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    Dg = discriminator(Gz, reuse=)\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "batch_size = 50\n",
    "z_dimensions = 100\n",
    "\n",
    "x_placeholder = tf.placeholder(\"float\", shape = [None, 28, 28, 1], name='x_placeholder')\n",
    "#the placeholder is for feeding images into the discriminator\n",
    "#Loss functions for GANs are more complex than the traditional CNN(where MSE or Hinge would do the trick)\n",
    "#think of a GAN as a zero sum minimax game\n",
    "#the generator is seeking to generate better and more convincing images\n",
    "#the discriminator seeks to become better at distinguishing real and false images\n",
    "\n",
    "Gz = generator(batch_size, z_dimensions)\n",
    "#Gz holds the generated images\n",
    "#g(z)\n",
    "Dx = discriminator(x_placeholder)\n",
    "# Dx holds the discriminators prediction probabilities\n",
    "#D(x)\n",
    "Dg = discriminator(Gz, reuse=True)\n",
    "#Dg holds the discriminator prediction probabilities for generated images\n",
    "#D(g(z))\n",
    "\n",
    "#The generator wants the discriminator to output a 1(positive...or fooled)\n",
    "#therefore we want to compute the loss between the Dg and label of 1\n",
    "#this can be done with the tf.nn.sigmoid_cross_entropy_with_logits function\n",
    "#This means the cross entropy loss will be taken between two arguments. The with_logits part\n",
    "#means that the function will operate on unscaled values....which means\n",
    "#instead of using a softmax function to squash the output activations to probability values\n",
    "#from 0 to 1 we just return the unscaled value of the matrix multiplication\n",
    "#the redduce mean function takes the mean of all the components in the matrix returned\n",
    "#by the cross entropy function. This is a way of reducing the loss\n",
    "#to a single scalar value, instead of a vector or matrix\n",
    "\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg, labels=tf.ones_like(Dg)))\n",
    "\n",
    "#The goal of the discriminator is to get the correct labels\n",
    "#output 1 for real image and 0 for generated images. WE want to compute the loss between\n",
    "#Dx and the correct label of 1 and the loss between Dg and the correct label of 0\n",
    "\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dx, labels=tf.fill([batch_size,1], 0.9)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg, labels=tf.zeros_like(Dg)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in tvars if 'd_' in var.name]\n",
    "g_vars = [var for var in tvars if 'g_' in var.name]\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse =False) as scope:\n",
    "    #Now we make the optimizers Adam uses learning rates and momentum\n",
    "    #we call adam's minimize function and specify the variables we want it to update\n",
    "    d_trainer_fake = tf.train.AdamOptimizer(0.0001).minimize(d_loss_fake, var_list=d_vars)\n",
    "    d_trainer_real = tf.train.AdamOptimizer(0.0001).minimize(d_loss_real, var_list=d_vars)\n",
    "    \n",
    "    #Train the generator\n",
    "    g_trainer = tf.train.AdamOptimizer(0.0001).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
